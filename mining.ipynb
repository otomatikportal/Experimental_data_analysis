{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "\n",
    "gdpdf = pd.read_csv('Raw_Data\\\\Economical\\\\gpd_data_incurrentUSD.csv')\n",
    "gdpdf = gdpdf.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "gdpdf = gdpdf.set_index('Country Code')\n",
    "country_codes = pd.read_csv('Raw_data\\\\country_codes.csv')\n",
    "gdpdf = gdpdf[gdpdf.index.isin(country_codes['alpha-3'])]\n",
    "gdpdf = gdpdf.map(lambda x: x / 1e6 if x is not None else x)\n",
    "gdpdf = gdpdf.round(2)\n",
    "cols_to_drop = [col for col in gdpdf.columns if int(col) < 2002]\n",
    "gdpdf = gdpdf.drop(columns=cols_to_drop)\n",
    "gdpdf.columns = gdpdf.columns.astype(int)\n",
    "\n",
    "co2ktdf = pd.read_csv('Raw_Data\\\\Environmental\\\\CO2_Emissions_ByCountry_KT.csv')\n",
    "co2ktdf = co2ktdf.drop(['Series Name','Series Code','Country Name'], axis=1)\n",
    "col_names = co2ktdf.columns.tolist()\n",
    "col_names[1:] = [name[:4] for name in col_names[1:]]\n",
    "co2ktdf.columns = col_names\n",
    "co2ktdf = co2ktdf.set_index('Country Code')\n",
    "co2ktdf = co2ktdf[co2ktdf.index.isin(country_codes['alpha-3'])]\n",
    "co2ktdf = co2ktdf.apply(pd.to_numeric, errors='coerce')\n",
    "cols_to_drop = [col for col in co2ktdf.columns if int(col) < 2002]\n",
    "co2ktdf = co2ktdf.drop(columns=cols_to_drop)\n",
    "co2ktdf.columns = co2ktdf.columns.astype(int)\n",
    "\n",
    "lawindex = pd.read_csv('Raw_Data\\\\Legal\\\\Law_Index.csv')\n",
    "lawindex = lawindex.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "lawindex = lawindex.set_index('Country Code')\n",
    "lawindex = lawindex[lawindex.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in lawindex.columns if int(col) < 2002]\n",
    "lawindex = lawindex.drop(columns=cols_to_drop)\n",
    "lawindex.columns = lawindex.columns.astype(int)\n",
    "\n",
    "politicalstab = pd.read_csv('Raw_Data\\\\Political\\\\political_stability_index.csv')\n",
    "politicalstab = politicalstab.drop(['Series Name','Series Code','Country Name'], axis=1)\n",
    "col_names = politicalstab.columns.tolist()\n",
    "col_names[1:] = [name[:4] for name in col_names[1:]]\n",
    "politicalstab.columns = col_names\n",
    "politicalstab = politicalstab.set_index('Country Code')\n",
    "politicalstab = politicalstab[politicalstab.index.isin(country_codes['alpha-3'])]\n",
    "politicalstab = politicalstab.apply(pd.to_numeric, errors='coerce')\n",
    "cols_to_drop = [col for col in politicalstab.columns if int(col) < 2002]\n",
    "politicalstab = politicalstab.drop(columns=cols_to_drop)\n",
    "politicalstab.columns = politicalstab.columns.astype(int)\n",
    "\n",
    "ginindex = pd.read_csv('Raw_Data\\\\Social\\\\Gini_Index_(0good-100bad).csv')\n",
    "ginindex = ginindex.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "ginindex = ginindex.set_index('Country Code')\n",
    "ginindex = ginindex[ginindex.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in ginindex.columns if int(col) < 2002]\n",
    "ginindex = ginindex.drop(columns=cols_to_drop)\n",
    "ginindex.columns = ginindex.columns.astype(int)\n",
    "\n",
    "techexpenditure1 = pd.read_csv('Raw_Data\\\\Technological\\\\API_GB.XPD.RSDV.GD.ZS_DS2_en_csv_v2_4.csv')\n",
    "techexpenditure1 = techexpenditure1.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "techexpenditure1 = techexpenditure1.set_index('Country Code')\n",
    "techexpenditure1 = techexpenditure1[techexpenditure1.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in techexpenditure1.columns if int(col) < 2002]\n",
    "techexpenditure1 = techexpenditure1.drop(columns=cols_to_drop)\n",
    "techexpenditure1.columns = techexpenditure1.columns.astype(int)\n",
    "techexpenditure = techexpenditure1 * gdpdf\n",
    "\n",
    "\n",
    "# Step 1\n",
    "# Fetches the country data series from the dataframes and returns them as a list of series\n",
    "def get_country_serie_list(country_code):\n",
    "    countrydf1 = gdpdf.loc[country_code].rename('Gdp'+' '+country_code)\n",
    "    countrydf2 = co2ktdf.loc[country_code].rename('CO2'+' '+country_code)\n",
    "    # countrydf6 = techexpenditure.loc[country_code].rename('Techexpenditure'+' '+country_code) \n",
    "    countrydf3 = lawindex.loc[country_code].rename('Lawindex'+' '+country_code)\n",
    "    countrydf4 = politicalstab.loc[country_code].rename('Politicalstab'+' '+country_code)\n",
    "    countrydf5 = ginindex.loc[country_code].rename('Ginindex'+' '+country_code)\n",
    "    \n",
    "    country_serie_list = [countrydf1, countrydf2, countrydf3, countrydf4, countrydf5]\n",
    "    return country_serie_list\n",
    "\n",
    "# Step 2\n",
    "# This function shortens the list of series to equal lenght aligned with the shortest serie\n",
    "def shorten_series(series_list):\n",
    "    first_valid_indices = [s.first_valid_index() for s in series_list]\n",
    "    max_index = max(first_valid_indices)\n",
    "    shortened_series_list = [s.loc[max_index:] for s in series_list]\n",
    "    return shortened_series_list\n",
    "\n",
    "# Step 3\n",
    "# Change method if required!\n",
    "def interpolator(serie_list):\n",
    "    interpolated_serie_list = [serie.interpolate(method='linear') for serie in serie_list]\n",
    "    return interpolated_serie_list\n",
    "\n",
    "# Step 4\n",
    "# Detrend the first two series in the list (which are gdp and co2)\n",
    "def detrend_first_two(series_list):\n",
    "    detrended_series_list = series_list.copy()\n",
    "    for i in range(2):  # Loop over the first two series\n",
    "        detrended_array = signal.detrend(series_list[i].values)\n",
    "        detrended_series_list[i] = pd.Series(detrended_array, \n",
    "                                             index=series_list[i].index, \n",
    "                                             name=series_list[i].name)\n",
    "        return detrended_series_list\n",
    "\n",
    "# Step 5\n",
    "# This function scales the series in the list\n",
    "def scale_series_list(series_list):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_series_list = []  \n",
    "    for df_serie in series_list:\n",
    "        # Scale the data\n",
    "        scaled_serie = pd.Series(scaler.fit_transform(df_serie.values.reshape(-1, 1)).flatten(), index=df_serie.index)\n",
    "        scaled_serie.name = df_serie.name  # Preserve the name\n",
    "        scaled_series_list.append(scaled_serie)\n",
    "    \n",
    "    return scaled_series_list\n",
    "\n",
    "\n",
    "\n",
    "def plot_with_trendlines(df):\n",
    "    \"\"\"\n",
    "    Plots each column in the DataFrame as a subplot with a trendline and displays the trendline equation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data to plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(6, 10))\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        # Plot the original data\n",
    "        df[col].plot(style='o-', ax=axes[i], title=col)\n",
    "        \n",
    "        # Calculate the trendline\n",
    "        z = np.polyfit(df.index, df[col], 1)\n",
    "        p = np.poly1d(z)\n",
    "        \n",
    "        # Get the trendline equation as a string\n",
    "        trendline_eq = f\"y = {z[0]:.2f}x + {z[1]:.2f}\"\n",
    "        \n",
    "        # Plot the trendline\n",
    "        plt.sca(axes[i])\n",
    "        plt.plot(df.index, p(df.index), \"r--\", label='Trend')\n",
    "        \n",
    "        # Set x-axis ticks as integers with intervals of 3\n",
    "        axes[i].set_xticks(np.arange(min(df.index), max(df.index)+1, 3))\n",
    "        \n",
    "        # Annotate the trendline equation\n",
    "        plt.text(0.05, 0.95, trendline_eq, transform=axes[i].transAxes, fontsize=9,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='red', facecolor='white'))\n",
    "        \n",
    "        # Add a legend\n",
    "        # axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_with_trendlines(fin_table_4)\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import shapiro, kstest, norm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def assess_normality(df):\n",
    "    \"\"\"\n",
    "    Performs Shapiro-Wilk and Kolmogorov-Smirnov tests on each column of the DataFrame.\n",
    "    Also generates Q-Q plots and histograms for visual assessment of normality.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data to test.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for column in df.columns:\n",
    "        # Shapiro-Wilk Test\n",
    "        stat_shapiro, p_value_shapiro = shapiro(df[column])\n",
    "        # Kolmogorov-Smirnov Test\n",
    "        stat_ks, p_value_ks = kstest(df[column], 'norm', args=(df[column].mean(), df[column].std()))\n",
    "        \n",
    "        results[column] = {\n",
    "            'Shapiro-Wilk Stat': stat_shapiro,\n",
    "            'Shapiro-Wilk P-Value': p_value_shapiro,\n",
    "            'Kolmogorov-Smirnov Stat': stat_ks,\n",
    "            'Kolmogorov-Smirnov P-Value': p_value_ks\n",
    "        }\n",
    "        \n",
    "        # Q-Q plot\n",
    "        sm.qqplot(df[column], line='s')\n",
    "        plt.title(f\"Q-Q Plot for {column}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Histogram\n",
    "        #plt.figure()\n",
    "        #df[column].hist(bins=20, edgecolor='black')\n",
    "        #plt.title(f\"Histogram for {column}\")\n",
    "        #plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "# results_df = assess_normality(your_dataframe)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(techexpenditure1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_non_null = gdpdf.count().sum()\n",
    "print(\"Total non-null values in gdpdf dataframe:\", total_non_null)\n",
    "\n",
    "total_non_null = co2ktdf.count().sum()\n",
    "print(\"Total non-null values in co2ktdf dataframe:\", total_non_null)\n",
    "\n",
    "total_non_null = lawindex.count().sum()\n",
    "print(\"Total non-null values in lawindex dataframe:\", total_non_null)\n",
    "\n",
    "total_non_null = politicalstab.count().sum()\n",
    "print(\"Total non-null values in politicalstab dataframe:\", total_non_null)\n",
    "\n",
    "total_non_null = ginindex.count().sum()\n",
    "print(\"Total non-null values in ginindex dataframe:\", total_non_null)\n",
    "\n",
    "total_non_null = techexpenditure.count().sum()\n",
    "print(\"Total non-null values in techexpenditure dataframe:\", total_non_null)\n",
    "\n",
    "row_count = gdpdf.shape[0]\n",
    "print(\"Row count:\", row_count)\n",
    "\n",
    "display(gdpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_code = 'USA'\n",
    "fin_table_list = get_country_serie_list(fin_code)\n",
    "fin_table = pd.concat(fin_table_list, axis=1)\n",
    "fin_table.index.name = 'Year' \n",
    "display(fin_table)\n",
    "\n",
    "fin_table_list = shorten_series(fin_table_list)\n",
    "fin_table_2 = pd.concat(fin_table_list, axis=1)\n",
    "fin_table_2.index.name = 'Year'\n",
    "display(fin_table_2)\n",
    "\n",
    "\n",
    "fin_table_list = interpolator(fin_table_list)\n",
    "fin_table_3 = pd.concat(fin_table_list, axis=1)\n",
    "fin_table_3.index.name = 'Year'\n",
    "display(fin_table_3)\n",
    "plot_with_trendlines(fin_table_3)\n",
    "\n",
    "fin_table_list = detrend_first_two(fin_table_list)\n",
    "fin_table_4 = pd.concat(fin_table_list, axis=1)\n",
    "fin_table_4.index.name = 'Year'\n",
    "display(fin_table_4)\n",
    "\n",
    "\n",
    "fin_table_list = scale_series_list(fin_table_list)\n",
    "fin_table_5 = pd.concat(fin_table_list, axis=1)\n",
    "fin_table_5.index.name = 'Year'\n",
    "display(fin_table_5)\n",
    "\n",
    "results_df = assess_normality(fin_table_5)\n",
    "display(results_df.tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUTINE CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_2 = 'FIN'\n",
    "country_series_list = scale_series_list(detrend_first_two(interpolator(shorten_series(get_country_serie_list(country_code_2)))))\n",
    "country_dfusa = pd.concat(country_series_list, axis=1)\n",
    "display(country_dfusa)\n",
    "country_dfusa.plot(subplots=True, figsize=(20, 20))\n",
    "\n",
    "print(country_dfusa.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUTINE CHECK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scale_series_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m country_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m country_series_list \u001b[38;5;241m=\u001b[39m \u001b[43mscale_series_list\u001b[49m(detrend_first_two((shorten_series(get_country_serie_list(country_code)))))\n\u001b[0;32m      3\u001b[0m country_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(country_series_list, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst 5 rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scale_series_list' is not defined"
     ]
    }
   ],
   "source": [
    "country_code = 'USA'\n",
    "country_series_list = scale_series_list(detrend_first_two((shorten_series(get_country_serie_list(country_code)))))\n",
    "country_df = pd.concat(country_series_list, axis=1)\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(country_df.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(country_df.tail())\n",
    "\n",
    "country_df.plot(subplots=False, figsize=(18, 5),style=['o-','o-','o-','o-','o-'], title=country_code)\n",
    "\n",
    "country_fin_matrix = country_df.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.heatmap(country_fin_matrix, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_kendall = country_df.corr(method='kendall')\n",
    "correlation_matrix_spearman = country_df.corr(method='spearman')\n",
    "correlation_matrix_pearson = country_df.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix_list = [correlation_matrix_kendall, correlation_matrix_spearman, correlation_matrix_pearson]\n",
    "correlation_methods = ['Kendall', 'Spearman', 'Pearson']\n",
    "\n",
    "for i, correlation_matrix in enumerate(correlation_matrix_list):\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "    plt.title(f'Correlation Matrix ({correlation_methods[i]})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha3_codes = country_codes['alpha-3'].tolist()\n",
    "failed_codes = []  # List to keep track of failed alpha-3 codes\n",
    "\n",
    "display(alpha3_codes)\n",
    "\n",
    "dataframes = []  # List to store series converted into dataframes\n",
    "failed_codes = []  # List to keep track of failed alpha-3 codes\n",
    "\n",
    "for country_code in alpha3_codes:\n",
    "    try:\n",
    "        country_series_list = scale_series_list(detrend_first_two(interpolator(shorten_series(get_country_serie_list(country_code)))))\n",
    "        country_df = pd.concat(country_series_list, axis=1)\n",
    "        print(country_df.columns[0], country_df.index[0])\n",
    "        dataframes.append(country_df)  # Append the dataframe to the list\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process alpha-3 code: {country_code}. Error: {str(e)}\")\n",
    "        failed_codes.append(country_code)\n",
    "\n",
    "print(f\"Failed alpha-3 codes: {failed_codes}\")\n",
    "\n",
    "display(len(dataframes))\n",
    "\n",
    "alpha3_codes = [code for code in alpha3_codes if code not in failed_codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(failed_codes))\n",
    "print(\"failed in\")\n",
    "print(len(alpha3_codes))\n",
    "# trust me this is not bad not all apha-3 codes are existing as countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "correlation_matrices = []\n",
    "\n",
    "for df in dataframes:\n",
    "    corr_matrix = df.corr(method = 'pearson')\n",
    "    correlation_matrices.append(corr_matrix)\n",
    "\n",
    "index_list = []\n",
    "values_list = []\n",
    "\n",
    "# Iterate over the correlation matrices\n",
    "for corr_matrix in correlation_matrices:\n",
    "    # Iterate over the items in the matrix\n",
    "    for pair, value in corr_matrix.unstack().items():\n",
    "        # Add the pair to the index list and the value to the values list\n",
    "        index_list.append(pair)\n",
    "        values_list.append(value)\n",
    "\n",
    "# Create a MultiIndex from the index list\n",
    "index = pd.MultiIndex.from_tuples(index_list, names=['Variable 1', 'Variable 2'])\n",
    "\n",
    "# Create a Series with the MultiIndex and the values list\n",
    "s = pd.Series(values_list, index=index)\n",
    "\n",
    "display(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = s != 1\n",
    "\n",
    "\n",
    "# Apply the masks to the Series\n",
    "filtered_s = s[mask1] # filtered out values = 1\n",
    "# filtered_s = filtered_s[filtered_s.index.get_level_values('Variable 1') < filtered_s.index.get_level_values('Variable 2')]\n",
    "\n",
    "extract = filtered_s.sort_values(ascending=False)\n",
    "\n",
    "extract.to_excel('outputs\\\\correlation_matrix.xlsx')\n",
    "\n",
    "display(extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_alpha3 = pd.read_csv('world-bank-income-groups.csv')\n",
    "income_alpha3_filtered = income_alpha3[income_alpha3['Code'].isin(alpha3_codes)]\n",
    "income_alpha3_filtered = income_alpha3_filtered[income_alpha3_filtered['Year'] == 2020]\n",
    "income_alpha3_filtered = income_alpha3_filtered[income_alpha3_filtered['Code'].isin(alpha3_codes)]\n",
    "income_alpha3_filtered = income_alpha3_filtered.reset_index(drop=True)\n",
    "display(income_alpha3_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(extract)\n",
    "extract_1 = extract.reset_index()\n",
    "extract_1 = extract_1.rename(columns={0: 'Values'})\n",
    "display(extract_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_1['Code'] = extract_1['Variable 2'].str[-3:]\n",
    "merged_df = extract_1.merge(income_alpha3_filtered[['Code', 'Entity',\"World Bank's income classification\"]], on='Code', how='left')\n",
    "merged_df['Variable 1'] = merged_df['Variable 1'].str.split(' ', expand=True)[0]\n",
    "merged_df['Variable 2'] = merged_df['Variable 2'].str.split(' ', expand=True)[0]\n",
    "display(merged_df)\n",
    "merged_df.to_excel('outputs\\\\correlation_matrix_with_income_groups.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_clusters = pd.read_csv('for_clustering_correlations.csv')\n",
    "display(for_clusters)\n",
    "for_clusters = for_clusters.dropna()\n",
    "display(for_clusters)\n",
    "for_clusters = for_clusters.set_index('Country Code', drop=True)\n",
    "display(for_clusters)\n",
    "print(for_clusters.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bins = [-1,-0.799,-0.599,-0.399,-0.199,0, 0.199, 0.399, 0.599, 0.799, 1.000]  # Define your bin edges\n",
    "labels = [-5,-4,-3,-2,-1,1,2,3,4,5]  # Define labels for the bins\n",
    "# 'Very Strong Inverse', 'Strong Inverse','Moderate Inverse', 'Low Inverse','Very Low Inverse', 'Very Low','Low','Moderate','Strong','Very Strong'\n",
    "# List of columns you want to bin\n",
    "columns_to_bin = for_clusters[0:9]\n",
    "\n",
    "binned_df = pd.DataFrame()\n",
    "\n",
    "# Apply binning to each column\n",
    "for column in columns_to_bin:\n",
    "    binned_df[f'binned_{column}'] = pd.cut(for_clusters[column], bins=bins, labels=labels)\n",
    "\n",
    "\n",
    "binned_df = binned_df.merge(income_alpha3_filtered[['Code', \"World Bank's income classification\"]],\n",
    "                            left_index=True,\n",
    "                            right_on='Code',\n",
    "                            how='left')\n",
    "binned_df.set_index('Code', inplace=True)\n",
    "\n",
    "mapping = {'Low-income countries': 1, 'Lower-middle-income countries': 2, 'Upper-middle-income countries': 3, 'High-income countries': 4}\n",
    "binned_df[\"World Bank's income classification\"] = binned_df[\"World Bank's income classification\"].replace(mapping)\n",
    "\n",
    "\n",
    "binned_df.dropna(inplace=True)\n",
    "display(binned_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4).fit(binned_df)\n",
    "\n",
    "binned_df['Cluster'] = kmeans.predict(binned_df)\n",
    "\n",
    "display(binned_df)\n",
    "\n",
    "binned_df.to_excel('outputs\\\\binned_clusterlabeled_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cluster_comparison[\"World Bank's income classification\"].dtype)\n",
    "\n",
    "cluster_comparison_clean = cluster_comparison.dropna()\n",
    "\n",
    "cluster_comparison_clean[\"World Bank's income classification\"] = cluster_comparison_clean[\"World Bank's income classification\"].astype(int)\n",
    "\n",
    "display(cluster_comparison_clean)\n",
    "\n",
    "cluster_comparison_clean.groupby(['Cluster', \"World Bank's income classification\"]).size().unstack().plot(kind='bar', stacked=True)\n",
    "cluster_comparison_clean.groupby([\"World Bank's income classification\",'Cluster' ]).size().unstack().plot(kind='bar', stacked=True)\n",
    "cluster_comparison_clean.groupby(['Cluster','Cluster' ]).size().unstack().plot(kind='bar', stacked=True)\n",
    "cluster_comparison_clean.groupby([\"World Bank's income classification\",\"World Bank's income classification\" ]).size().unstack().plot(kind='bar', stacked=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

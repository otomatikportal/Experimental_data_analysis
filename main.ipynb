{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, kstest, norm\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_codes = pd.read_csv('Raw_data\\\\country_codes.csv')\n",
    "display(country_codes.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdpdf = pd.read_csv('Raw_Data\\\\Economical\\\\gpd_data_incurrentUSD.csv')\n",
    "gdpdf = gdpdf.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "gdpdf = gdpdf.set_index('Country Code')\n",
    "gdpdf = gdpdf[gdpdf.index.isin(country_codes['alpha-3'])]\n",
    "gdpdf = gdpdf.map(lambda x: x / 1e6 if x is not None else x)\n",
    "gdpdf = gdpdf.round(2)\n",
    "cols_to_drop = [col for col in gdpdf.columns if int(col) < 2002]\n",
    "gdpdf = gdpdf.drop(columns=cols_to_drop)\n",
    "gdpdf.columns = gdpdf.columns.astype(int)\n",
    "display(gdpdf.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2ktdf = pd.read_csv('Raw_Data\\\\Environmental\\\\CO2_Emissions_ByCountry_KT.csv')\n",
    "co2ktdf = co2ktdf.drop(['Series Name','Series Code','Country Name'], axis=1)\n",
    "col_names = co2ktdf.columns.tolist()\n",
    "col_names[1:] = [name[:4] for name in col_names[1:]]\n",
    "co2ktdf.columns = col_names\n",
    "co2ktdf = co2ktdf.set_index('Country Code')\n",
    "co2ktdf = co2ktdf[co2ktdf.index.isin(country_codes['alpha-3'])]\n",
    "co2ktdf = co2ktdf.apply(pd.to_numeric, errors='coerce')\n",
    "cols_to_drop = [col for col in co2ktdf.columns if int(col) < 2002]\n",
    "co2ktdf = co2ktdf.drop(columns=cols_to_drop)\n",
    "co2ktdf.columns = co2ktdf.columns.astype(int)\n",
    "display(co2ktdf.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lawindex = pd.read_csv('Raw_Data\\\\Legal\\\\Law_Index.csv')\n",
    "lawindex = lawindex.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "lawindex = lawindex.set_index('Country Code')\n",
    "lawindex = lawindex[lawindex.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in lawindex.columns if int(col) < 2002]\n",
    "lawindex = lawindex.drop(columns=cols_to_drop)\n",
    "lawindex.columns = lawindex.columns.astype(int)\n",
    "display(lawindex.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicalstab = pd.read_csv('Raw_Data\\\\Political\\\\political_stability_index.csv')\n",
    "politicalstab = politicalstab.drop(['Series Name','Series Code','Country Name'], axis=1)\n",
    "col_names = politicalstab.columns.tolist()\n",
    "col_names[1:] = [name[:4] for name in col_names[1:]]\n",
    "politicalstab.columns = col_names\n",
    "politicalstab = politicalstab.set_index('Country Code')\n",
    "politicalstab = politicalstab[politicalstab.index.isin(country_codes['alpha-3'])]\n",
    "politicalstab = politicalstab.apply(pd.to_numeric, errors='coerce')\n",
    "cols_to_drop = [col for col in politicalstab.columns if int(col) < 2002]\n",
    "politicalstab = politicalstab.drop(columns=cols_to_drop)\n",
    "politicalstab.columns = politicalstab.columns.astype(int)\n",
    "display(politicalstab.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ginindex = pd.read_csv('Raw_Data\\\\Social\\\\Gini_Index_(0good-100bad).csv')\n",
    "ginindex = ginindex.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "ginindex = ginindex.set_index('Country Code')\n",
    "ginindex = ginindex[ginindex.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in ginindex.columns if int(col) < 2002]\n",
    "ginindex = ginindex.drop(columns=cols_to_drop)\n",
    "ginindex.columns = ginindex.columns.astype(int)\n",
    "display(ginindex.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techexpenditure1 = pd.read_csv('Raw_Data\\\\Technological\\\\API_GB.XPD.RSDV.GD.ZS_DS2_en_csv_v2_4.csv')\n",
    "techexpenditure1 = techexpenditure1.drop(['Country Name','Indicator Name','Indicator Code'], axis=1)\n",
    "techexpenditure1 = techexpenditure1.set_index('Country Code')\n",
    "techexpenditure1 = techexpenditure1[techexpenditure1.index.isin(country_codes['alpha-3'])]\n",
    "cols_to_drop = [col for col in techexpenditure1.columns if int(col) < 2002]\n",
    "techexpenditure1 = techexpenditure1.drop(columns=cols_to_drop)\n",
    "techexpenditure1.columns = techexpenditure1.columns.astype(int)\n",
    "techexpenditure = techexpenditure1 * gdpdf\n",
    "display(techexpenditure.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non null check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_non_null_counts(dataframes):\n",
    "    for name, df in dataframes.items():\n",
    "        total_non_null = df.count().sum()\n",
    "        print(f\"Total non-null values in {name} dataframe:\", total_non_null)\n",
    "\n",
    "# Dictionary of dataframes\n",
    "dataframes = {\n",
    "    'gdpdf': gdpdf,\n",
    "    'co2ktdf': co2ktdf,\n",
    "    'lawindex': lawindex,\n",
    "    'politicalstab': politicalstab,\n",
    "    'ginindex': ginindex,\n",
    "    'techexpenditure': techexpenditure\n",
    "}\n",
    "\n",
    "print_non_null_counts(dataframes)\n",
    "\n",
    "# For row count, you can still keep it separate if you only need it for 'gdpdf'\n",
    "row_count = gdpdf.shape[0]\n",
    "print(\"Row count:\", row_count)\n",
    "\n",
    "# Display the 'gdpdf' dataframe\n",
    "display(gdpdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the preprocess functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Fetches the country data series from the dataframes and returns them as a list of series\n",
    "def get_country_serie_list(country_code):\n",
    "    countrydf1 = gdpdf.loc[country_code].rename('Gdp'+' '+country_code)\n",
    "    countrydf2 = co2ktdf.loc[country_code].rename('CO2'+' '+country_code)\n",
    "    # countrydf6 = techexpenditure.loc[country_code].rename('Techexpenditure'+' '+country_code) \n",
    "    countrydf3 = lawindex.loc[country_code].rename('Lawindex'+' '+country_code)\n",
    "    countrydf4 = politicalstab.loc[country_code].rename('Politicalstab'+' '+country_code)\n",
    "    countrydf5 = ginindex.loc[country_code].rename('Ginindex'+' '+country_code)\n",
    "    \n",
    "    country_serie_list = [countrydf1, countrydf2, countrydf3, countrydf4, countrydf5]\n",
    "    return country_serie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# This function shortens the list of series to equal lenght aligned with the shortest serie\n",
    "def shorten_series(series_list):\n",
    "    first_valid_indices = [s.first_valid_index() for s in series_list]\n",
    "    max_index = max(first_valid_indices)\n",
    "    shortened_series_list = [s.loc[max_index:] for s in series_list]\n",
    "    return shortened_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Change method if required!\n",
    "def interpolator(serie_list):\n",
    "    interpolated_serie_list = [serie.interpolate(method='linear') for serie in serie_list]\n",
    "    return interpolated_serie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Detrend the first two series in the list (which are gdp and co2)\n",
    "def detrend_first_two(series_list):\n",
    "    detrended_series_list = series_list.copy()\n",
    "    for i in range(2):  # Loop over the first two series\n",
    "        detrended_array = signal.detrend(series_list[i].values)\n",
    "        detrended_series_list[i] = pd.Series(detrended_array, \n",
    "                                             index=series_list[i].index, \n",
    "                                             name=series_list[i].name)\n",
    "        return detrended_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# This function scales the series in the list\n",
    "def scale_series_list(series_list):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_series_list = []  \n",
    "    for df_serie in series_list:\n",
    "        # Scale the data\n",
    "        scaled_serie = pd.Series(scaler.fit_transform(df_serie.values.reshape(-1, 1)).flatten(), index=df_serie.index)\n",
    "        scaled_serie.name = df_serie.name  # Preserve the name\n",
    "        scaled_series_list.append(scaled_serie)\n",
    "    \n",
    "    return scaled_series_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_trendlines(df):\n",
    "    \"\"\"\n",
    "    Plots each column in the DataFrame as a subplot with a trendline and displays the trendline equation.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data to plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=len(df.columns), ncols=1, figsize=(6, 10))\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        # Plot the original data\n",
    "        df[col].plot(style='o-', ax=axes[i], title=col)\n",
    "        \n",
    "        # Calculate the trendline\n",
    "        z = np.polyfit(df.index, df[col], 1)\n",
    "        p = np.poly1d(z)\n",
    "        \n",
    "        # Get the trendline equation as a string\n",
    "        trendline_eq = f\"y = {z[0]:.2f}x + {z[1]:.2f}\"\n",
    "        \n",
    "        # Plot the trendline\n",
    "        plt.sca(axes[i])\n",
    "        plt.plot(df.index, p(df.index), \"r--\", label='Trend')\n",
    "        \n",
    "        # Set x-axis ticks as integers with intervals of 3\n",
    "        axes[i].set_xticks(np.arange(min(df.index), max(df.index)+1, 3))\n",
    "        \n",
    "        # Annotate the trendline equation\n",
    "        plt.text(0.05, 0.95, trendline_eq, transform=axes[i].transAxes, fontsize=9,\n",
    "                 verticalalignment='top', bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor='red', facecolor='white'))\n",
    "        \n",
    "        # Add a legend\n",
    "        # axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# plot_with_trendlines(fin_table_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_normality(df):\n",
    "    \"\"\"\n",
    "    Performs Shapiro-Wilk and Kolmogorov-Smirnov tests on each column of the DataFrame.\n",
    "    Also generates Q-Q plots and histograms for visual assessment of normality.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data to test.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for column in df.columns:\n",
    "        # Shapiro-Wilk Test\n",
    "        stat_shapiro, p_value_shapiro = shapiro(df[column])\n",
    "        # Kolmogorov-Smirnov Test\n",
    "        stat_ks, p_value_ks = kstest(df[column], 'norm', args=(df[column].mean(), df[column].std()))\n",
    "        \n",
    "        results[column] = {\n",
    "            'Shapiro-Wilk Stat': stat_shapiro,\n",
    "            'Shapiro-Wilk P-Value': p_value_shapiro,\n",
    "            'Kolmogorov-Smirnov Stat': stat_ks,\n",
    "            'Kolmogorov-Smirnov P-Value': p_value_ks\n",
    "        }\n",
    "        \n",
    "        # Q-Q plot\n",
    "        sm.qqplot(df[column], line='s')\n",
    "        plt.title(f\"Q-Q Plot for {column}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Histogram\n",
    "        #plt.figure()\n",
    "        #df[column].hist(bins=20, edgecolor='black')\n",
    "        #plt.title(f\"Histogram for {column}\")\n",
    "        #plt.show()\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "# results_df = assess_normality(your_dataframe)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha3_codes = country_codes['alpha-3'].tolist()\n",
    "failed_codes = []  # List to keep track of failed alpha-3 codes\n",
    "\n",
    "display(alpha3_codes)\n",
    "\n",
    "dataframes = []  # List to store series converted into dataframes\n",
    "failed_codes = []  # List to keep track of failed alpha-3 codes\n",
    "\n",
    "for country_code in alpha3_codes:\n",
    "    try:\n",
    "        country_series_list = scale_series_list(detrend_first_two(interpolator(shorten_series(get_country_serie_list(country_code)))))\n",
    "        country_df = pd.concat(country_series_list, axis=1)\n",
    "        print(country_df.columns[0], country_df.index[0])\n",
    "        dataframes.append(country_df)  # Append the dataframe to the list\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process alpha-3 code: {country_code}. Error: {str(e)}\")\n",
    "        failed_codes.append(country_code)\n",
    "\n",
    "print(f\"Failed alpha-3 codes: {failed_codes}\")\n",
    "\n",
    "display(len(dataframes))\n",
    "\n",
    "alpha3_codes = [code for code in alpha3_codes if code not in failed_codes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply correlation iteratively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrices = []\n",
    "\n",
    "for df in dataframes:\n",
    "    corr_matrix = df.corr(method = 'pearson')\n",
    "    correlation_matrices.append(corr_matrix)\n",
    "\n",
    "index_list = []\n",
    "values_list = []\n",
    "\n",
    "# Iterate over the correlation matrices\n",
    "for corr_matrix in correlation_matrices:\n",
    "    # Iterate over the items in the matrix\n",
    "    for pair, value in corr_matrix.unstack().items():\n",
    "        # Add the pair to the index list and the value to the values list\n",
    "        index_list.append(pair)\n",
    "        values_list.append(value)\n",
    "\n",
    "# Create a MultiIndex from the index list\n",
    "index = pd.MultiIndex.from_tuples(index_list, names=['Variable 1', 'Variable 2'])\n",
    "\n",
    "# Create a Series with the MultiIndex and the values list\n",
    "correlation_output = pd.Series(values_list, index=index)\n",
    "\n",
    "display(correlation_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosmetic upgrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexfixed = correlation_output.reset_index()\n",
    "indexfixed.columns = ['Variable 1', 'Variable 2', 'Value']\n",
    "display(indexfixed)\n",
    "\n",
    "# Filter out pairs where the variables are the same Gdp-Gdp or CO2-CO2 Etc.\n",
    "indexfixed_filtered = indexfixed[(indexfixed['Variable 1'] != indexfixed['Variable 2'])]\n",
    "display(indexfixed_filtered.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import income classifications and filter it aligned with the current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_alpha3 = pd.read_csv('world-bank-income-groups.csv')\n",
    "income_alpha3_filtered = income_alpha3[income_alpha3['Code'].isin(alpha3_codes)]\n",
    "income_alpha3_filtered = income_alpha3_filtered[income_alpha3_filtered['Year'] == 2020]\n",
    "income_alpha3_filtered = income_alpha3_filtered[income_alpha3_filtered['Code'].isin(alpha3_codes)]\n",
    "income_alpha3_filtered = income_alpha3_filtered.reset_index(drop=True)\n",
    "income_alpha3_filtered = income_alpha3_filtered.rename(columns={\"World Bank's income classification\" : 'Income Group'})\n",
    "\n",
    "#venezuela missing so add it\n",
    "new_row = {'Entity': 'Venezuela', 'Code': 'VEN', 'Year': '2020', 'Income Group': 'Upper-middle-income countries'}\n",
    "income_alpha3_filtered = pd.concat([income_alpha3_filtered, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "display(income_alpha3_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with additional data (income group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning 'Code' using .loc\n",
    "indexfixed_filtered.loc[:, 'Code'] = indexfixed_filtered['Variable 2'].str[-3:]\n",
    "\n",
    "# Merging dataframes\n",
    "final_all_corr = indexfixed_filtered.merge(income_alpha3_filtered[['Code', 'Entity', \"Income Group\"]], on='Code', how='left')\n",
    "\n",
    "# Splitting 'Variable 1' and 'Variable 2' and assigning the first part using .loc\n",
    "final_all_corr.loc[:, 'Variable 1'] = final_all_corr['Variable 1'].str.split(' ', expand=True)[0]\n",
    "final_all_corr.loc[:, 'Variable 2'] = final_all_corr['Variable 2'].str.split(' ', expand=True)[0]\n",
    "\n",
    "display(final_all_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change or eliminate the interval here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_verystrong = final_all_corr[(final_all_corr['Value'] >= 0.6) | (final_all_corr['Value'] <= -0.6)]\n",
    "display(strong_verystrong.head(5))\n",
    "\n",
    "var1 = ['Gdp', 'Politicalstab', 'Ginindex']\n",
    "var2 = ['CO2', 'Lawindex']\n",
    "\n",
    "strong_verystrong_filtered = strong_verystrong[strong_verystrong['Variable 1'].isin(var1) & strong_verystrong['Variable 2'].isin(var2)]\n",
    "display(strong_verystrong_filtered)\n",
    "# strong_verystrong_filtered.to_excel('outputs\\\\strong_verystrong_filtered.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations = strong_verystrong_filtered.groupby(['Variable 1', 'Variable 2']).size().reset_index().drop(columns=0)\n",
    "display(unique_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby_variablepair = strong_verystrong_filtered.groupby(['Variable 1', 'Variable 2', 'Income Group'])\n",
    "list_of_dfs = [group for _, group in groupedby_variablepair]\n",
    "\n",
    "display((list_of_dfs[0]))\n",
    "display(len(list_of_dfs))\n",
    "\n",
    "processed_listof_dfs = []\n",
    "\n",
    "for df in list_of_dfs:\n",
    "    df['Variable Pair'] = df['Variable 1'] + '-' + df['Variable 2']\n",
    "    df.drop(columns=['Variable 1', 'Variable 2'], inplace=True)\n",
    "    processed_listof_dfs.append(df)\n",
    "    \n",
    "display(strong_verystrong_filtered)\n",
    "\n",
    "for df in processed_listof_dfs:\n",
    "    print(df['Variable Pair'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hopefullyfinal = strong_verystrong_filtered.copy()\n",
    "hopefullyfinal['Variable Pair'] = hopefullyfinal['Variable 1'] + '-' + hopefullyfinal['Variable 2']\n",
    "hopefullyfinal.drop(columns=['Variable 1', 'Variable 2','Entity'], inplace=True)\n",
    "display(hopefullyfinal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named 'hopefullyfinal'\n",
    "# Replace 'Value', 'Income Group', and 'Variable Pair' with actual column names\n",
    "\n",
    "# Create histograms for each 'Variable Pair'\n",
    "unique_variable_pairs = hopefullyfinal['Variable Pair'].unique()\n",
    "num_bins = 20\n",
    "\n",
    "# Define colors for each income group\n",
    "income_group_colors = {\n",
    "    'Low-income countries': 'red',\n",
    "    'Lower-middle-income countries': 'orange',\n",
    "    'Upper-middle-income countries': 'yellow',\n",
    "    'High-income countries': 'green'\n",
    "}\n",
    "\n",
    "# Define the bin edges for the histogram/bar chart\n",
    "bin_edges = np.linspace(-1, 1, num_bins+1)\n",
    "\n",
    "# Create a figure and a grid of subplots\n",
    "fig, axs = plt.subplots(len(unique_variable_pairs), figsize=(12, 4*len(unique_variable_pairs)))\n",
    "\n",
    "for ax, var_pair in zip(axs, unique_variable_pairs):\n",
    "    subset_df = hopefullyfinal[hopefullyfinal['Variable Pair'] == var_pair]\n",
    "    \n",
    "    # Plot stacked bar charts for each income group\n",
    "    bottom = np.zeros(num_bins)\n",
    "    for income_group, color in income_group_colors.items():\n",
    "        group_subset_df = subset_df[subset_df['Income Group'] == income_group]\n",
    "        \n",
    "        # Calculate histogram values without plotting\n",
    "        hist_values, _ = np.histogram(group_subset_df['Value'], bins=bin_edges)\n",
    "        \n",
    "        # Plot as bar chart, stacking on top of previous bars\n",
    "        ax.bar(bin_edges[:-1], hist_values, width=0.1, bottom=bottom, edgecolor='black', alpha=0.7, label=income_group, color=color)\n",
    "        bottom += hist_values  # Update bottom array to stack bars\n",
    "    \n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Stacked Bar Chart for {var_pair}')\n",
    "    ax.set_xticks(np.arange(-1, 1.1, 0.1))\n",
    "    ax.set_xticklabels([round(-1 + i * 0.1, 1) for i in range(21)], rotation=90)  # Customize x-axis ticks\n",
    "    ax.grid(axis='y')\n",
    "    ax.set_yticks(np.arange(0, max(bottom) + 1, 1))  # Customize y-axis ticks\n",
    "    ax.legend()  # Show legend with income group labels\n",
    "    ax.set_ylim(0, max(bottom) * 1.1)  # Adjust the multiplier as needed to add more or less offset\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_serie_list = get_country_serie_list('USA')\n",
    "usa_serie_list = scale_series_list(detrend_first_two(interpolator(shorten_series(usa_serie_list))))\n",
    "usa_df = pd.concat(usa_serie_list, axis=1)\n",
    "display(usa_df)\n",
    "\n",
    "my_series = usa_serie_list[0]\n",
    "\n",
    "# Assuming you have a valid usa_df DataFrame\n",
    "my_series.plot(subplots=True, figsize=(10, 3), style='o-', colormap='viridis', sharex=False)\n",
    "\n",
    "final_all_corr[(final_all_corr['Code'] == 'USA') & (final_all_corr['Variable 1'] == 'Gdp')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Given data (replace with your actual data)\n",
    "\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(usa_df)\n",
    "\n",
    "# Calculate moving averages for each series\n",
    "window_size = 3\n",
    "for col in ['CO2 USA', 'Lawindex USA', 'Politicalstab USA', 'Ginindex USA']:\n",
    "    df[f'{col}_MA'] = df[col].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "# Calculate correlations between GDP and other series\n",
    "correlations = df[['Gdp USA', 'CO2 USA_MA', 'Lawindex USA_MA', 'Politicalstab USA_MA', 'Ginindex USA_MA']].corr()\n",
    "weights = correlations.loc['Gdp USA'][1:]  # Exclude GDP itself\n",
    "weights /= weights.sum()  # Normalize weights\n",
    "\n",
    "# Combine moving averages using correlation values as weights\n",
    "df['Gdp_Forecast'] = np.dot(df[['CO2 USA_MA', 'Lawindex USA_MA', 'Politicalstab USA_MA', 'Ginindex USA_MA']], weights)\n",
    "\n",
    "# Extrapolate to 2026 (assuming linear trend)\n",
    "last_year = df.index.max()\n",
    "slope = (df['Gdp_Forecast'].iloc[-1] - df['Gdp_Forecast'].iloc[-2]) / (last_year - 2022)\n",
    "forecast_2026 = df['Gdp_Forecast'].iloc[-1] + slope * (2026 - last_year)\n",
    "print(f\"GDP Forecast for 2026: {forecast_2026:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
